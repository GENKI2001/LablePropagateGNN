import torch
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer
import random
from collections import defaultdict
from torch_geometric.utils import to_networkx
import networkx as nx
from sklearn.preprocessing import StandardScaler
import torch.nn.functional as F

def display_node_features(data, adj_matrix, one_hot_labels, dataset_name, max_hops=2, sample_nodes=None):
    """
    ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´é‡ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
    
    Args:
        data: PyTorch Geometric ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        adj_matrix: éš£æ¥è¡Œåˆ—
        one_hot_labels: ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«
        dataset_name (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        max_hops (int): æœ€å¤§hopæ•°
        sample_nodes (list): è¡¨ç¤ºã™ã‚‹ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: [0, 10, 50, 100, 200]ï¼‰
    """
    
    if sample_nodes is None:
        sample_nodes = [0, 10, 50, 100, 200]
    
    print(f"\n=== {dataset_name} ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã®ç¢ºèª ===")
    print(f"ç‰¹å¾´é‡ã®å½¢çŠ¶: {data.x.shape}")
    print(f"ã‚¯ãƒ©ã‚¹æ•°: {one_hot_labels.shape[1]}")
    print(f"æœ€å¤§hopæ•°: {max_hops}")
    
    # unknownã‚¯ãƒ©ã‚¹ã®æƒ…å ±ã‚’è¡¨ç¤º
    unknown_mask = ~data.train_mask
    if unknown_mask.sum() > 0:
        print(f"unknownã‚¯ãƒ©ã‚¹: ã‚ã‚Šï¼ˆãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ãƒãƒ¼ãƒ‰ç”¨ï¼‰")
        print(f"unknownãƒãƒ¼ãƒ‰æ•°: {unknown_mask.sum().item()}")
    else:
        print(f"unknownã‚¯ãƒ©ã‚¹: ãªã—")
    
    for hop in range(1, max_hops + 1):
        print(f"{hop}-hopç‰¹å¾´é‡ã®æ¬¡å…ƒ: {one_hot_labels.shape[1]}")

    # ã„ãã¤ã‹ã®ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´é‡ã‚’è¡¨ç¤º
    for node_idx in sample_nodes:
        if node_idx < data.num_nodes:
            # æ­£ã—ã„æ¬¡æ•°ã‚’è¨ˆç®—ï¼ˆéš£æ¥è¡Œåˆ—ã‹ã‚‰ï¼‰
            degree = adj_matrix[node_idx].sum().item()
            
            print(f"\nãƒãƒ¼ãƒ‰ {node_idx}:")
            print(f"  å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«: {data.y[node_idx].item()}")
            print(f"  ãƒãƒ¼ãƒ‰ã®ç¨®é¡: {'è¨“ç·´' if data.train_mask[node_idx] else 'ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼'}")
            print(f"  ãƒãƒ¼ãƒ‰ã®æ¬¡æ•°: {degree}")
            
            # å„hopã®ç‰¹å¾´é‡ã‚’è¡¨ç¤º
            start_idx = 0
            for hop in range(1, max_hops + 1):
                end_idx = start_idx + one_hot_labels.shape[1]
                hop_feat = data.x[node_idx, start_idx:end_idx].cpu().numpy()
                print(f"  {hop}-hopéš£æ¥ãƒãƒ¼ãƒ‰ã®å¹³å‡ç‰¹å¾´é‡: {hop_feat}")
                start_idx = end_idx
            
            # çµåˆã•ã‚ŒãŸç‰¹å¾´é‡
            combined_feat = data.x[node_idx].cpu().numpy()
            print(f"  çµåˆã•ã‚ŒãŸç‰¹å¾´é‡: {combined_feat}")

def get_feature_info(data, one_hot_labels, max_hops=2):
    """
    ç‰¹å¾´é‡ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    
    Args:
        data: PyTorch Geometric ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        one_hot_labels: ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«
        max_hops (int): æœ€å¤§hopæ•°
    
    Returns:
        dict: ç‰¹å¾´é‡ã®æƒ…å ±ã‚’å«ã‚€è¾æ›¸
    """
    return {
        'feature_dim': data.x.shape[1],
        'num_classes': one_hot_labels.shape[1],
        'max_hops': max_hops,
        'hop_dims': [one_hot_labels.shape[1]] * max_hops
    }

def create_pca_features(data, device, pca_components=50, original_features=None):
    """
    ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã«å¯¾ã—ã¦PCAã§æ¬¡å…ƒåœ§ç¸®ã—ãŸæ–°ãŸãªç‰¹å¾´é‡ã‚’è¿”ã™é–¢æ•°

    Args:
        data (torch_geometric.data.Data): PyGã®ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        device (torch.device): ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆcuda ã¾ãŸã¯ cpuï¼‰
        pca_components (int): åœ§ç¸®å¾Œã®æ¬¡å…ƒæ•°
        original_features (torch.Tensor or None): åœ§ç¸®å¯¾è±¡ã®ç‰¹å¾´é‡ï¼ˆæŒ‡å®šãŒãªã‘ã‚Œã° data.xï¼‰

    Returns:
        data: PCAç‰¹å¾´é‡ãŒè¨­å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        pca_features (torch.Tensor): PCAåœ§ç¸®å¾Œã®ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆ[num_nodes, pca_components]ï¼‰
        pca: PCAã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    print(f"PCAç‰¹å¾´é‡ã‚’ä½œæˆä¸­...ï¼ˆåœ§ç¸®æ¬¡å…ƒ: {pca_components}ï¼‰")

    # å…¥åŠ›ç‰¹å¾´é‡ã‚’æŒ‡å®š or ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ data.x
    if original_features is None:
        if not hasattr(data, 'x') or data.x is None:
            raise ValueError("original_features ãŒ None ã®å ´åˆã€data.x ãŒå¿…è¦ã§ã™ã€‚")
        features = data.x.cpu().numpy()
    else:
        features = original_features.cpu().numpy()

    # PCAã«ã‚ˆã‚‹æ¬¡å…ƒåœ§ç¸®
    pca = PCA(n_components=pca_components, random_state=42)
    reduced_features = pca.fit_transform(features)

    print(f"PCAå®Œäº†: å…ƒã®æ¬¡å…ƒæ•° = {features.shape[1]}, åœ§ç¸®å¾Œ = {pca_components}")
    print(f"ç´¯ç©å¯„ä¸ç‡ï¼ˆexplained variance ratioï¼‰: {pca.explained_variance_ratio_.sum():.4f}")

    # PCAç‰¹å¾´é‡ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    pca_features = torch.tensor(reduced_features, dtype=torch.float32).to(device)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç‰¹å¾´é‡ã‚’æ›´æ–°
    data.x = pca_features

    return data, pca_features, pca

def create_label_features(data, device, max_hops=2, calc_neighbor_label_features=True, temperature=1.0, label_smoothing=0.):
    print(f"ç¾åœ¨ã®ç‰¹å¾´é‡ã®å½¢çŠ¶: {data.x.shape}")

    # ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä½œæˆ
    train_labels = data.y[data.train_mask].cpu().numpy().reshape(-1, 1)
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    encoder.fit(train_labels)
    all_labels = data.y.cpu().numpy().reshape(-1, 1)
    one_hot_labels = encoder.transform(all_labels)

    # ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ã‚’0ã«è¨­å®šï¼ˆå¸¸ã«Trueã®å‹•ä½œï¼‰
    one_hot_labels[~data.train_mask.cpu().numpy()] = 0

    # === âœ… ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨ ===
    if label_smoothing > 0.0:
        num_classes = one_hot_labels.shape[1]
        one_hot_labels = one_hot_labels * (1 - label_smoothing) + (label_smoothing / num_classes)
        print(f"ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚’é©ç”¨: Îµ = {label_smoothing}")

    edge_index = data.edge_index.cpu()
    num_nodes = data.num_nodes
    adj_matrix = torch.zeros((num_nodes, num_nodes), dtype=torch.float32)
    adj_matrix[edge_index[0], edge_index[1]] = 1.0
    adj_matrix[edge_index[1], edge_index[0]] = 1.0
    combined_features = data.x
    neighbor_label_features = None

    if calc_neighbor_label_features:
        one_hot_labels_tensor = torch.tensor(one_hot_labels, dtype=torch.float32)
        hop_features_list = []
        edge_index_np = edge_index.numpy()

        A = torch.zeros((num_nodes, num_nodes))
        A[edge_index[0], edge_index[1]] = 1
        A[edge_index[1], edge_index[0]] = 1
        A = A.bool()
        A.fill_diagonal_(False)

        # å„hopã¾ã§ã®åˆ°é”å¯èƒ½æ€§ã‚’è¿½è·¡
        reachable_nodes = torch.zeros((num_nodes, num_nodes), dtype=torch.bool)
        
        for hop in range(1, max_hops + 1):
            if hop == 1:
                # 1hop: ç›´æ¥éš£æ¥ãƒãƒ¼ãƒ‰ã®ã¿
                mask = A.clone()
                reachable_nodes = mask.clone()
            else:
                # 2hopä»¥ä¸Š: å‰ã®hopã¾ã§ã®åˆ°é”å¯èƒ½æ€§ã‚’é™¤å¤–
                # ç¾åœ¨ã®hopã§åˆ°é”å¯èƒ½ãªãƒãƒ¼ãƒ‰ã‚’è¨ˆç®—
                current_reachable = torch.matmul(reachable_nodes.float(), A.float()).bool()
                # å‰ã®hopã¾ã§ã®åˆ°é”å¯èƒ½æ€§ã‚’é™¤å¤–
                mask = current_reachable & (~reachable_nodes)
                # åˆ°é”å¯èƒ½æ€§ã‚’æ›´æ–°
                reachable_nodes = reachable_nodes | current_reachable
            
            # å„ãƒãƒ¼ãƒ‰ã«ã¤ã„ã¦ã€ãã®hopã§åˆ°é”å¯èƒ½ãªéš£æ¥ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ã‚’é›†ç´„
            neighbor_labels = mask.float() @ one_hot_labels_tensor
            hop_features = F.softmax(neighbor_labels / temperature, dim=1)
            hop_features_list.append(hop_features)
            
            print(f"  {hop}hop: {mask.sum().item()}å€‹ã®æ¥ç¶šï¼ˆå‰ã®hopã‚’é™¤å¤–ï¼‰")

        neighbor_label_features = torch.cat(hop_features_list, dim=1)
        combined_features = torch.cat([data.x, neighbor_label_features], dim=1)
        print(f"  - ç”Ÿã®ç‰¹å¾´é‡: {data.x.shape[1]}æ¬¡å…ƒ")
        print(f"  - ãƒ©ãƒ™ãƒ«åˆ†å¸ƒç‰¹å¾´é‡: {neighbor_label_features.shape[1]}æ¬¡å…ƒ")
    else:
        print("éš£æ¥ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡ã¯çµåˆã—ã¾ã›ã‚“")

    return adj_matrix, one_hot_labels, neighbor_label_features

def create_positional_random_walk_label_features(data, device, walk_length=4, use_train_only=True):
    from torch_sparse import spmm
    from torch_geometric.utils import add_self_loops, degree

    num_nodes = data.num_nodes
    y = data.y.cpu().numpy()

    print(f"\n=== é †åºä»˜ããƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ç‰¹å¾´é‡ä½œæˆ ===")
    print(f"ãƒãƒ¼ãƒ‰æ•°: {num_nodes}")
    print(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯é•·: {walk_length}")
    print(f"è¨“ç·´ãƒãƒ¼ãƒ‰ã®ã¿ä½¿ç”¨: {use_train_only}")

    # === ãƒ©ãƒ™ãƒ«ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆåŒ– ===
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    encoder.fit(y[data.train_mask.cpu().numpy()].reshape(-1, 1))
    one_hot_labels = encoder.transform(y.reshape(-1, 1))
    num_classes = one_hot_labels.shape[1]
    print(f"ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")
    
    one_hot_labels[~data.train_mask.cpu().numpy()] = 0
    print(f"ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ã‚’0ã«è¨­å®š")
    
    Y = torch.tensor(one_hot_labels, dtype=torch.float32).to(device)  # [N, C]

    # === æ­£è¦åŒ–éš£æ¥è¡Œåˆ—ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ï¼‰ã‚’ä½œæˆ ===
    edge_index, _ = add_self_loops(data.edge_index)  # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã‚ã‚Š
    row, col = edge_index
    deg = degree(row, num_nodes, dtype=torch.float32)  # å‡ºæ¬¡æ•°
    deg_inv = 1.0 / deg
    deg_inv[deg_inv == float('inf')] = 0

    norm = deg_inv[row]  # è»¢ç½®ãªã— (row-normalized)
    A_hat = torch.sparse_coo_tensor(
        indices=edge_index,
        values=norm,
        size=(num_nodes, num_nodes)
    ).to(device)

    print(f"æ­£è¦åŒ–éš£æ¥è¡Œåˆ—ä½œæˆå®Œäº†: {A_hat.shape}")

    # === å„ hop ã«ãŠã‘ã‚‹ä¼æ’­çµæœã‚’è¨˜éŒ² ===
    H = Y
    label_hops = []
    print(f"\nå„hopã§ã®ç‰¹å¾´é‡æ¬¡å…ƒ:")
    for t in range(1, walk_length + 1):
        H = torch.sparse.mm(A_hat, H)  # t-stepä¼æ’­
        label_hops.append(H)
        print(f"  {t}hop: {H.shape[1]}æ¬¡å…ƒ (ã‚¯ãƒ©ã‚¹æ•°: {num_classes})")

    # === [N, walk_length, C] ã«å¤‰æ›ã—ã¦çµåˆ ===
    position_label_tensor = torch.stack(label_hops, dim=1)  # [N, T, C]
    flattened = position_label_tensor.view(num_nodes, -1)   # [N, T*C]

    print(f"\n=== ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ç‰¹å¾´é‡ã®è©³ç´° ===")
    print(f"å…ƒã®ç‰¹å¾´é‡æ¬¡å…ƒ: {data.x.shape[1]}")
    print(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ç‰¹å¾´é‡æ¬¡å…ƒ: {flattened.shape[1]}")
    print(f"  - å„hop: {num_classes}æ¬¡å…ƒ Ã— {walk_length}hop = {num_classes * walk_length}æ¬¡å…ƒ")
    print(f"  - ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶: {position_label_tensor.shape} â†’ å¹³å¦åŒ–: {flattened.shape}")

    # === data.x ã«çµåˆ ===
    original_x_shape = data.x.shape
    
    return position_label_tensor

def compute_extended_structural_features(data, device,
                                         include_degree=True,
                                         include_clustering=True,
                                         include_triangle=True,
                                         include_depth=False,
                                         include_avg_neighbor_degree=True,
                                         include_pagerank=True,
                                         include_eigenvector=True,
                                         include_kcore=True,
                                         include_l2_stats=True
                                         ):
    """
    å„ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã¦æ§‹é€ çš„ç‰¹å¾´é‡ã‚’è¨ˆç®—ã—ã€data.x ã«çµåˆã™ã‚‹é–¢æ•°ï¼ˆæ‹¡å¼µç‰ˆï¼‰

    å«ã¾ã‚Œã‚‹æ§‹é€ çš„ç‰¹å¾´é‡ï¼ˆä»»æ„é¸æŠï¼‰:
        - degree: ãƒãƒ¼ãƒ‰ã®æ¬¡æ•°
        - clustering: ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•°
        - triangle: ä¸‰è§’å½¢æ•°
        - depth: sourceãƒãƒ¼ãƒ‰ã‹ã‚‰ã®BFSæ·±ã•
        - avg_neighbor_degree: éš£æ¥ãƒãƒ¼ãƒ‰ã®å¹³å‡æ¬¡æ•°
        - pagerank: PageRankã‚¹ã‚³ã‚¢
        - eigenvector: å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ä¸­å¿ƒæ€§ï¼ˆå½±éŸ¿åŠ›ï¼‰
        - kcore: ã‚³ã‚¢ç•ªå·ï¼ˆä¸­æ ¸çš„å¯†åº¦ãƒ¬ãƒ™ãƒ«ï¼‰
        - l2_stats: L2ãƒãƒ«ãƒ å·®åˆ†çµ±è¨ˆé‡ï¼ˆå¹³å‡ã€æ¨™æº–åå·®ã€æœ€å¤§å€¤ã€æœ€å°å€¤ï¼‰

    Returns:
        data: ç‰¹å¾´é‡ãŒçµåˆã•ã‚ŒãŸ PyG ãƒ‡ãƒ¼ã‚¿
        structural_features: æ§‹é€ çš„ç‰¹å¾´é‡ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆtorch.Tensorï¼‰
    """
    
    num_nodes = data.num_nodes

    # PyTorch Geometric ãƒ‡ãƒ¼ã‚¿ã‚’ NetworkX ã‚°ãƒ©ãƒ•ã«å¤‰æ›ï¼ˆç„¡å‘ï¼‰
    G = to_networkx(data, to_undirected=True)
    
    # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã‚’å‰Šé™¤ï¼ˆNetworkXã®ä¸€éƒ¨ã®é–¢æ•°ã§å¿…è¦ï¼‰
    G.remove_edges_from(nx.selfloop_edges(G))

    # ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚’ã“ã“ã«æ ¼ç´ï¼ˆNumPyé…åˆ—ã¨ã—ã¦ï¼‰
    features = []

    if include_degree:
        # ğŸ”¢ å„ãƒãƒ¼ãƒ‰ã®æ¬¡æ•°ï¼ˆdegreeï¼‰: å˜ç´”ãªæ¥ç¶šæ•°
        degree_feat = np.array([val for _, val in sorted(G.degree())], dtype=np.float32).reshape(-1, 1)
        features.append(degree_feat)

    if include_clustering:
        # ğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•°: ãƒãƒ¼ãƒ‰ã®è¿‘å‚å†…ã§ä¸‰è§’å½¢ãŒå½¢æˆã•ã‚Œã¦ã„ã‚‹å‰²åˆ
        clustering_dict = nx.clustering(G)
        clustering_feat = np.array([clustering_dict[i] for i in range(num_nodes)], dtype=np.float32).reshape(-1, 1)
        features.append(clustering_feat)

    if include_triangle:
        # ğŸ”º ä¸‰è§’å½¢æ•°: ãƒãƒ¼ãƒ‰ãŒå±ã™ã‚‹ä¸‰è§’å½¢æ§‹é€ ã®æ•°ï¼ˆå±€æ‰€å¯†åº¦ã®æŒ‡æ¨™ï¼‰
        triangle_dict = nx.triangles(G)
        triangle_feat = np.array([triangle_dict[i] for i in range(num_nodes)], dtype=np.float32).reshape(-1, 1)
        features.append(triangle_feat)

    if include_depth:
        # ğŸ§­ ãƒãƒ¼ãƒ‰0ã‚’ãƒ«ãƒ¼ãƒˆã¨ã—ãŸã¨ãã® BFS æ·±ã•ï¼ˆå‚è€ƒæƒ…å ±ã¨ã—ã¦ï¼‰
        depth_dict = nx.single_source_shortest_path_length(G, source=0)
        depth_feat = np.zeros((num_nodes, 1), dtype=np.float32)
        for i in range(num_nodes):
            depth_feat[i, 0] = depth_dict.get(i, 0)
        features.append(depth_feat)

    if include_avg_neighbor_degree:
        # ğŸ”„ éš£æ¥ãƒãƒ¼ãƒ‰ã®å¹³å‡æ¬¡æ•°: å‘¨å›²ã®ãƒãƒ¼ãƒ‰ã®å¯†åº¦ã®å¹³å‡
        avg_deg_dict = nx.average_neighbor_degree(G)
        avg_deg_feat = np.array([avg_deg_dict[i] for i in range(num_nodes)], dtype=np.float32).reshape(-1, 1)
        features.append(avg_deg_feat)

    if include_pagerank:
        # ğŸŒ PageRank: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªé‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆã‚¹ãƒ‘ãƒ æ¤œå‡ºãªã©ã§ã‚‚æœ‰ç”¨ï¼‰
        pr = nx.pagerank(G)
        pr_feat = np.array([pr[i] for i in range(num_nodes)], dtype=np.float32).reshape(-1, 1)
        features.append(pr_feat)

    if include_eigenvector:
        try:
            # ğŸ“ˆ å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ä¸­å¿ƒæ€§: å½±éŸ¿åŠ›ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰ã¨ã¤ãªãŒã£ã¦ã„ã‚‹ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ã„
            eig = nx.eigenvector_centrality_numpy(G)
            eig_feat = np.array([eig[i] for i in range(num_nodes)], dtype=np.float32).reshape(-1, 1)
            features.append(eig_feat)
        except:
            print("eigenvector_centrality ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    if include_kcore:
        # ğŸ—ï¸ k-core ç•ªå·: ãƒãƒ¼ãƒ‰ãŒå±ã™ã‚‹æœ€å¤§ã® k-core ã® k å€¤ï¼ˆä¸­æ ¸æ€§ã®æŒ‡æ¨™ï¼‰
        kcore = nx.core_number(G)
        kcore_feat = np.array([kcore[i] for i in range(num_nodes)], dtype=np.float32).reshape(-1, 1)
        features.append(kcore_feat)

    if include_l2_stats:
        # ======== L2ãƒãƒ«ãƒ å·®åˆ†çµ±è¨ˆé‡ã‚’è¿½åŠ  ========
        x_np = data.x.cpu().numpy()  # å…¨ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«
        l2_stats = np.zeros((num_nodes, 4), dtype=np.float32)  # mean, std, max, min

        for i in range(num_nodes):
            neighbors = list(G.neighbors(i))
            if not neighbors:
                continue
            neighbor_feats = x_np[neighbors]
            diff_norms = np.linalg.norm(x_np[i] - neighbor_feats, axis=1)
            l2_stats[i, 0] = diff_norms.mean()
            l2_stats[i, 1] = diff_norms.std()
            l2_stats[i, 2] = diff_norms.max()
            l2_stats[i, 3] = diff_norms.min()

        # æ­£è¦åŒ–ï¼ˆå¹³å‡0ãƒ»åˆ†æ•£1ï¼‰
        l2_stats = StandardScaler().fit_transform(l2_stats)
        l2_tensor = torch.tensor(l2_stats, dtype=torch.float32).to(device)
        features.append(l2_tensor)

    # ğŸ’¡ å…¨æ§‹é€ ç‰¹å¾´é‡ã‚’çµåˆï¼ˆNumPyé…åˆ— [num_nodes, total_features]ï¼‰
    structural_features = np.concatenate(features, axis=1)

    # âš–ï¸ ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ï¼ˆå¹³å‡0ãƒ»åˆ†æ•£1ï¼‰â†’ æ¯”è¼ƒå¯èƒ½ã«ã™ã‚‹ãŸã‚é‡è¦
    scaler = StandardScaler()
    structural_features = scaler.fit_transform(structural_features)

    # ğŸ” NumPy â†’ torch.Tensor ã¸å¤‰æ›ï¼ˆãƒ‡ãƒã‚¤ã‚¹ã¸é€ã‚‹ï¼‰
    structural_features = torch.tensor(structural_features, dtype=torch.float32).to(device)

    # ğŸ“ æ—¢å­˜ã®ç‰¹å¾´é‡ï¼ˆdata.xï¼‰ã«æ§‹é€ ç‰¹å¾´é‡ã‚’çµåˆ
    data.x = torch.cat([data.x, structural_features], dim=1)

    return data, structural_features

def create_similarity_based_edges(features, threshold=0.5, device='cpu'):
    """
    ç‰¹å¾´é‡ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚’ä½œæˆã™ã‚‹é–¢æ•°
    
    Args:
        features (torch.Tensor): ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ [num_nodes, feature_dim]ï¼ˆç”Ÿã®ç‰¹å¾´é‡ã¾ãŸã¯ãƒ©ãƒ™ãƒ«åˆ†å¸ƒç‰¹å¾´é‡ãªã©ï¼‰
        threshold (float): ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆ0.0-1.0ï¼‰
        device (str): ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
    
    Returns:
        torch.Tensor: æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ [2, num_new_edges]
        torch.Tensor: æ–°ã—ã„éš£æ¥è¡Œåˆ— [num_nodes, num_nodes]
        int: ä½œæˆã•ã‚ŒãŸã‚¨ãƒƒã‚¸æ•°
    """
    print(f"\n=== é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã‚¨ãƒƒã‚¸ä½œæˆ ===")
    print(f"ç‰¹å¾´é‡å½¢çŠ¶: {features.shape}")
    print(f"é¡ä¼¼åº¦é–¾å€¤: {threshold}")
    
    num_nodes = features.shape[0]
    
    # ç‰¹å¾´é‡ã‚’æ­£è¦åŒ–ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã®ãŸã‚ï¼‰
    features_normalized = F.normalize(features, p=2, dim=1)
    
    # å…¨ãƒãƒ¼ãƒ‰é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
    similarity_matrix = torch.mm(features_normalized, features_normalized.t())
    
    # å¯¾è§’æˆåˆ†ï¼ˆè‡ªå·±é¡ä¼¼åº¦ï¼‰ã‚’0ã«è¨­å®š
    similarity_matrix.fill_diagonal_(0.0)
    
    # é–¾å€¤ã‚’è¶…ãˆã‚‹é¡ä¼¼åº¦ã‚’æŒã¤ãƒšã‚¢ã‚’æŠ½å‡º
    edge_mask = similarity_matrix > threshold
    
    # ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’è€ƒæ…®ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    upper_triangle_mask = torch.triu(torch.ones_like(similarity_matrix), diagonal=1).bool()
    edge_mask = edge_mask & upper_triangle_mask
    
    # ã‚¨ãƒƒã‚¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    edge_indices = torch.nonzero(edge_mask, as_tuple=False)
    
    # åŒæ–¹å‘ã®ã‚¨ãƒƒã‚¸ã‚’ä½œæˆï¼ˆç„¡å‘ã‚°ãƒ©ãƒ•ã®ãŸã‚ï¼‰
    if len(edge_indices) > 0:
        # å…ƒã®ã‚¨ãƒƒã‚¸
        source_nodes = edge_indices[:, 0]
        target_nodes = edge_indices[:, 1]
        
        # é€†æ–¹å‘ã®ã‚¨ãƒƒã‚¸
        reverse_source = target_nodes
        reverse_target = source_nodes
        
        # ä¸¡æ–¹å‘ã‚’çµåˆ
        all_source = torch.cat([source_nodes, reverse_source])
        all_target = torch.cat([target_nodes, reverse_target])
        
        new_edge_index = torch.stack([all_source, all_target], dim=0)
    else:
        # ã‚¨ãƒƒã‚¸ãŒãªã„å ´åˆ
        new_edge_index = torch.empty((2, 0), dtype=torch.long, device=device)
    
    # æ–°ã—ã„éš£æ¥è¡Œåˆ—ã‚’ä½œæˆ
    new_adj_matrix = torch.zeros((num_nodes, num_nodes), dtype=torch.float32, device=device)
    if len(new_edge_index) > 0:
        new_adj_matrix[new_edge_index[0], new_edge_index[1]] = 1.0
    
    num_new_edges = new_edge_index.shape[1] // 2 if len(new_edge_index) > 0 else 0
    
    print(f"ä½œæˆã•ã‚ŒãŸã‚¨ãƒƒã‚¸æ•°: {num_new_edges}")
    print(f"æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å½¢çŠ¶: {new_edge_index.shape}")
    print(f"æ–°ã—ã„éš£æ¥è¡Œåˆ—å½¢çŠ¶: {new_adj_matrix.shape}")
    
    # é¡ä¼¼åº¦ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    if len(similarity_matrix) > 0:
        valid_similarities = similarity_matrix[upper_triangle_mask]
        print(f"é¡ä¼¼åº¦çµ±è¨ˆ:")
        print(f"  å¹³å‡: {valid_similarities.mean():.4f}")
        print(f"  æ¨™æº–åå·®: {valid_similarities.std():.4f}")
        print(f"  æœ€å°å€¤: {valid_similarities.min():.4f}")
        print(f"  æœ€å¤§å€¤: {valid_similarities.max():.4f}")
        print(f"  é–¾å€¤ {threshold} ã‚’è¶…ãˆã‚‹ãƒšã‚¢æ•°: {edge_mask.sum().item()}")
    
    return new_edge_index, new_adj_matrix, num_new_edges


def create_similarity_based_edges_with_original(original_edge_index, features, 
                                              threshold=0.5, device='cpu', 
                                              combine_with_original=True):
    """
    ç‰¹å¾´é‡ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚’ä½œæˆã—ã€
    å…ƒã®ã‚¨ãƒƒã‚¸ã¨çµåˆã™ã‚‹é–¢æ•°
    
    Args:
        original_edge_index (torch.Tensor): å…ƒã®ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ [2, num_edges]
        features (torch.Tensor): ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ [num_nodes, feature_dim]ï¼ˆç”Ÿã®ç‰¹å¾´é‡ã¾ãŸã¯ãƒ©ãƒ™ãƒ«åˆ†å¸ƒç‰¹å¾´é‡ãªã©ï¼‰
        threshold (float): ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆ0.0-1.0ï¼‰
        device (str): ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
        combine_with_original (bool): å…ƒã®ã‚¨ãƒƒã‚¸ã¨çµåˆã™ã‚‹ã‹ã©ã†ã‹
    
    Returns:
        torch.Tensor: çµåˆã•ã‚ŒãŸã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ [2, num_total_edges]
        torch.Tensor: çµåˆã•ã‚ŒãŸéš£æ¥è¡Œåˆ— [num_nodes, num_nodes]
        int: å…ƒã®ã‚¨ãƒƒã‚¸æ•°
        int: æ–°ã—ãä½œæˆã•ã‚ŒãŸã‚¨ãƒƒã‚¸æ•°
        int: ç·ã‚¨ãƒƒã‚¸æ•°
    """
    print(f"\n=== é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã‚¨ãƒƒã‚¸ä½œæˆï¼ˆå…ƒã‚¨ãƒƒã‚¸çµåˆï¼‰ ===")
    print(f"å…ƒã®ã‚¨ãƒƒã‚¸æ•°: {original_edge_index.shape[1]}")
    print(f"ç‰¹å¾´é‡å½¢çŠ¶: {features.shape}")
    print(f"é¡ä¼¼åº¦é–¾å€¤: {threshold}")
    print(f"å…ƒã‚¨ãƒƒã‚¸ã¨çµåˆ: {combine_with_original}")
    
    num_nodes = features.shape[0]
    
    # æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚’ä½œæˆ
    new_edge_index, new_adj_matrix, num_new_edges = create_similarity_based_edges(
        features, threshold, device
    )
    
    if combine_with_original:
        # å…ƒã®ã‚¨ãƒƒã‚¸ã¨æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚’çµåˆ
        combined_edge_index = torch.cat([original_edge_index, new_edge_index], dim=1)
        
        # é‡è¤‡ã‚¨ãƒƒã‚¸ã‚’é™¤å»ï¼ˆå…ƒã®ã‚¨ãƒƒã‚¸ã¨æ–°ã—ã„ã‚¨ãƒƒã‚¸ãŒé‡è¤‡ã™ã‚‹å¯èƒ½æ€§ï¼‰
        edge_pairs = combined_edge_index.t()
        unique_edges, inverse_indices = torch.unique(edge_pairs, dim=0, return_inverse=True)
        combined_edge_index = unique_edges.t()
        
        # çµåˆã•ã‚ŒãŸéš£æ¥è¡Œåˆ—ã‚’ä½œæˆ
        combined_adj_matrix = torch.zeros((num_nodes, num_nodes), dtype=torch.float32, device=device)
        combined_adj_matrix[combined_edge_index[0], combined_edge_index[1]] = 1.0
        
        num_original_edges = original_edge_index.shape[1]
        num_total_edges = combined_edge_index.shape[1]
        num_actual_new_edges = num_total_edges - num_original_edges
        
        print(f"çµåˆçµæœ:")
        print(f"  å…ƒã®ã‚¨ãƒƒã‚¸æ•°: {num_original_edges}")
        print(f"  æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸã‚¨ãƒƒã‚¸æ•°: {num_actual_new_edges}")
        print(f"  ç·ã‚¨ãƒƒã‚¸æ•°: {num_total_edges}")
        print(f"  ã‚¨ãƒƒã‚¸å¢—åŠ ç‡: {num_actual_new_edges / num_original_edges * 100:.2f}%")
        
        return combined_edge_index, combined_adj_matrix, num_original_edges, num_actual_new_edges, num_total_edges
    else:
        # æ–°ã—ã„ã‚¨ãƒƒã‚¸ã®ã¿ã‚’è¿”ã™
        num_original_edges = original_edge_index.shape[1]
        num_total_edges = new_edge_index.shape[1]
        
        return new_edge_index, new_adj_matrix, num_original_edges, num_new_edges, num_total_edges

