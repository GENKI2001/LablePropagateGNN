import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict, Counter
from dataset_loader import load_dataset, get_supported_datasets
import warnings
import os
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

class LabelCorrelationAnalyzer:
    """
    éš£æ¥ãƒãƒ¼ãƒ‰é–“ã®ãƒ©ãƒ™ãƒ«ç›¸é–¢ã‚’åˆ†æãƒ»å¯è¦–åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, device=None):
        """
        åˆæœŸåŒ–
        
        Args:
            device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•é¸æŠï¼‰
        """
        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.results = {}
        
    def analyze_dataset(self, dataset_name, save_plots=True, output_dir='./'):
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«ç›¸é–¢ã‚’åˆ†æ
        
        Args:
            dataset_name (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            save_plots (bool): ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print(f"\n=== {dataset_name} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«ç›¸é–¢åˆ†æ ===")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        data, dataset = load_dataset(dataset_name, self.device)
        
        # åˆ†æå®Ÿè¡Œ
        analysis_result = self._analyze_label_correlations(data, dataset)
        
        # çµæœã‚’ä¿å­˜
        self.results[dataset_name] = analysis_result
        
        # å¯è¦–åŒ–
        self._visualize_correlations(dataset_name, analysis_result, save_plots, output_dir)
        
        return analysis_result
    
    def _analyze_label_correlations(self, data, dataset):
        """
        ãƒ©ãƒ™ãƒ«ç›¸é–¢ã‚’åˆ†æ
        
        Args:
            data: PyTorch Geometric ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            dataset: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            dict: åˆ†æçµæœ
        """
        # ã‚¨ãƒƒã‚¸æƒ…å ±ã‚’å–å¾—
        edge_index = data.edge_index.cpu().numpy()
        labels = data.y.cpu().numpy()
        
        # éš£æ¥ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ãƒšã‚¢ã‚’åé›†ï¼ˆç„¡å‘ã‚°ãƒ©ãƒ•ãªã®ã§é‡è¤‡ã‚’é™¤å»ï¼‰
        label_pairs = []
        unique_edges = set()
        
        for i in range(edge_index.shape[1]):
            src, dst = edge_index[:, i]
            # ç„¡å‘ã‚°ãƒ©ãƒ•ã®å ´åˆã€åŒã˜ã‚¨ãƒƒã‚¸ãŒ2å›æ ¼ç´ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§é‡è¤‡ã‚’é™¤å»
            edge_tuple = tuple(sorted([src, dst]))
            if edge_tuple not in unique_edges:
                unique_edges.add(edge_tuple)
                src_label = labels[src]
                dst_label = labels[dst]
                # é †åºã‚’çµ±ä¸€ï¼ˆå°ã•ã„ãƒ©ãƒ™ãƒ«ã‚’å…ˆã«ï¼‰
                if src_label <= dst_label:
                    label_pairs.append((src_label, dst_label))
                else:
                    label_pairs.append((dst_label, src_label))
        
        # ãƒ©ãƒ™ãƒ«ãƒšã‚¢ã®é »åº¦ã‚’è¨ˆç®—
        pair_counts = Counter(label_pairs)
        
        # å„ãƒ©ãƒ™ãƒ«ã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—
        label_counts = Counter(labels)
        total_nodes = len(labels)
        
        # ç›¸é–¢è¡Œåˆ—ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®é »åº¦ã®ã¿ï¼‰
        correlation_matrix = np.zeros((dataset.num_classes, dataset.num_classes))
        
        for (label1, label2), actual_count in pair_counts.items():
            correlation_matrix[label1, label2] = actual_count
            correlation_matrix[label2, label1] = actual_count  # å¯¾ç§°è¡Œåˆ—
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        total_edges_analyzed = len(label_pairs)
        unique_pairs = len(pair_counts)
        
        # æœ€ã‚‚é »ç¹ãªãƒ©ãƒ™ãƒ«ãƒšã‚¢ã‚’ç‰¹å®š
        most_frequent_pairs = sorted(
            [(pair, count) for pair, count in pair_counts.items()],
            key=lambda x: x[1], reverse=True
        )
        
        # æœ€ã‚‚ç¨€ãªãƒ©ãƒ™ãƒ«ãƒšã‚¢ã‚’ç‰¹å®š
        least_frequent_pairs = sorted(
            [(pair, count) for pair, count in pair_counts.items()],
            key=lambda x: x[1]
        )
        
        return {
            'correlation_matrix': correlation_matrix,
            'pair_counts': dict(pair_counts),
            'label_counts': dict(label_counts),
            'total_edges': total_edges_analyzed,
            'unique_pairs': unique_pairs,
            'most_frequent_pairs': most_frequent_pairs[:5],
            'least_frequent_pairs': least_frequent_pairs[:5],
            'dataset_info': {
                'num_nodes': data.num_nodes,
                'num_edges': len(unique_edges),  # é‡è¤‡ã‚’é™¤å»ã—ãŸå®Ÿéš›ã®ã‚¨ãƒƒã‚¸æ•°
                'num_classes': dataset.num_classes
            }
        }
    
    def _visualize_correlations(self, dataset_name, analysis_result, save_plots, output_dir):
        """
        ç›¸é–¢çµæœã‚’å¯è¦–åŒ–
        
        Args:
            dataset_name (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            analysis_result (dict): åˆ†æçµæœ
            save_plots (bool): ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆï¼ˆ2x2ã‹ã‚‰3ã¤ã®ãƒ—ãƒ­ãƒƒãƒˆã«å¤‰æ›´ï¼‰
        fig, axes = plt.subplots(1, 3, figsize=(24, 8))
        fig.suptitle(f'{dataset_name} Dataset: Label Correlation Analysis', fontsize=18, fontweight='bold')
        
        # 1. ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®æ£’ã‚°ãƒ©ãƒ•ï¼ˆå·¦ï¼‰
        label_counts = analysis_result['label_counts']
        labels = list(label_counts.keys())
        counts = list(label_counts.values())
        
        bars = axes[0].bar(labels, counts, color='skyblue', alpha=0.7)
        axes[0].set_title('Label Distribution', fontweight='bold', fontsize=14)
        axes[0].set_xlabel('Label', fontsize=12)
        axes[0].set_ylabel('Number of Nodes', fontsize=12)
        axes[0].grid(True, alpha=0.3)
        
        # ãƒãƒ¼ã®ä¸Šã«å€¤ã‚’è¡¨ç¤º
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            axes[0].text(bar.get_x() + bar.get_width()/2., height,
                        f'{count:,}', ha='center', va='bottom', fontsize=11)
        
        # 2. ç›¸é–¢è¡Œåˆ—ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆçœŸã‚“ä¸­ã€å¤§ããï¼‰
        correlation_matrix = analysis_result['correlation_matrix']
        num_classes = correlation_matrix.shape[0]
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        df_corr = pd.DataFrame(
            correlation_matrix,
            index=[f'Label {i}' for i in range(num_classes)],
            columns=[f'Label {i}' for i in range(num_classes)]
        )
        
        sns.heatmap(df_corr, annot=True, fmt='.0f', cmap='Blues', ax=axes[1], 
                   square=True, cbar_kws={'shrink': 0.8}, annot_kws={'size': 12})
        axes[1].set_title('Adjacent Node Label Pair Frequency', fontweight='bold', fontsize=14)
        axes[1].set_xlabel('Label', fontsize=12)
        axes[1].set_ylabel('Label', fontsize=12)
        
        # 3. ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®è©³ç´°æƒ…å ±ï¼ˆå³ï¼‰
        dataset_info = analysis_result['dataset_info']
        
        # Homophilyã‚’è¨ˆç®—
        total_edges = analysis_result['total_edges']
        same_label_edges = 0
        for (label1, label2), count in analysis_result['pair_counts'].items():
            if label1 == label2:
                same_label_edges += count
        
        homophily = same_label_edges / total_edges if total_edges > 0 else 0
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤ºï¼ˆãƒ‡ã‚¶ã‚¤ãƒ³æ”¹å–„ï¼‰
        info_text = f"""DATASET INFORMATION
{'='*35}
Nodes: {dataset_info['num_nodes']:,}
Edges: {dataset_info['num_edges']:,}
Classes: {dataset_info['num_classes']}
Density: {(2 * dataset_info['num_edges']) / (dataset_info['num_nodes'] * (dataset_info['num_nodes'] - 1)):.6f}
Homophily: {homophily:.4f}

LABEL DISTRIBUTION
{'='*35}"""
        for label, count in sorted(label_counts.items()):
            percentage = (count / dataset_info['num_nodes']) * 100
            info_text += f"\nLabel {label:2d}: {count:8,} ({percentage:6.1f}%)"
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ”¹å–„ï¼ˆæ–‡å­—ã‚µã‚¤ã‚ºã‚’å¤§ããï¼‰
        axes[2].text(0.05, 0.95, info_text, transform=axes[2].transAxes, 
                    fontsize=12, verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle='round,pad=0.8', facecolor='lightblue', 
                             alpha=0.9, edgecolor='navy', linewidth=2))
        axes[2].set_title('Dataset Details', fontweight='bold', fontsize=14)
        axes[2].axis('off')
        
        plt.tight_layout()
        
        if save_plots:
            # label_correlation_imagesãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
            save_dir = os.path.join(output_dir, 'label_correlation_images')
            os.makedirs(save_dir, exist_ok=True)
            
            plt.savefig(f'{save_dir}/{dataset_name}_label_correlation.png', 
                       dpi=300, bbox_inches='tight')
            print(f"ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_dir}/{dataset_name}_label_correlation.png")
        
        # plt.show()ã‚’å‰Šé™¤ã—ã¦ç”»åƒã‚’è¡¨ç¤ºã—ãªã„
        plt.close()  # ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã«ãƒ—ãƒ­ãƒƒãƒˆã‚’é–‰ã˜ã‚‹
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        self._print_statistics(dataset_name, analysis_result)
    
    def _print_statistics(self, dataset_name, analysis_result):
        """
        çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        
        Args:
            dataset_name (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            analysis_result (dict): åˆ†æçµæœ
        """
        print(f"\n=== {dataset_name} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ± ===")
        
        dataset_info = analysis_result['dataset_info']
        print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"  ãƒãƒ¼ãƒ‰æ•°: {dataset_info['num_nodes']:,}")
        print(f"  ã‚¨ãƒƒã‚¸æ•°: {dataset_info['num_edges']:,}")
        print(f"  ã‚¯ãƒ©ã‚¹æ•°: {dataset_info['num_classes']}")
        print(f"  åˆ†æã—ãŸã‚¨ãƒƒã‚¸æ•°: {analysis_result['total_edges']:,}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«ãƒšã‚¢æ•°: {analysis_result['unique_pairs']}")
        
        # ã‚°ãƒ©ãƒ•ã®å¯†åº¦ã‚’è¨ˆç®—
        density = (2 * dataset_info['num_edges']) / (dataset_info['num_nodes'] * (dataset_info['num_nodes'] - 1))
        print(f"  ã‚°ãƒ©ãƒ•å¯†åº¦: {density:.6f}")
        
        # Homophilyã‚’è¨ˆç®—
        total_edges = analysis_result['total_edges']
        same_label_edges = 0
        for (label1, label2), count in analysis_result['pair_counts'].items():
            if label1 == label2:
                same_label_edges += count
        
        homophily = same_label_edges / total_edges if total_edges > 0 else 0
        print(f"  åŒè³ªæ€§ (Homophily): {homophily:.4f}")
        
        print(f"\nğŸ·ï¸ ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        total_nodes = dataset_info['num_nodes']
        for label, count in sorted(analysis_result['label_counts'].items()):
            percentage = (count / total_nodes) * 100
            print(f"  ãƒ©ãƒ™ãƒ« {label}: {count:,} ãƒãƒ¼ãƒ‰ ({percentage:.1f}%)")
        
        # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®çµ±è¨ˆ
        label_counts_list = list(analysis_result['label_counts'].values())
        print(f"\nğŸ“ˆ ãƒ©ãƒ™ãƒ«åˆ†å¸ƒçµ±è¨ˆ:")
        print(f"  æœ€å¤§ãƒ©ãƒ™ãƒ«ã‚µã‚¤ã‚º: {max(label_counts_list):,} ãƒãƒ¼ãƒ‰")
        print(f"  æœ€å°ãƒ©ãƒ™ãƒ«ã‚µã‚¤ã‚º: {min(label_counts_list):,} ãƒãƒ¼ãƒ‰")
        print(f"  å¹³å‡ãƒ©ãƒ™ãƒ«ã‚µã‚¤ã‚º: {np.mean(label_counts_list):.1f} ãƒãƒ¼ãƒ‰")
        print(f"  ãƒ©ãƒ™ãƒ«ã‚µã‚¤ã‚ºã®æ¨™æº–åå·®: {np.std(label_counts_list):.1f}")
        
        # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡åº¦ã‚’è¨ˆç®—
        max_count = max(label_counts_list)
        min_count = min(label_counts_list)
        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
        print(f"  ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”: {imbalance_ratio:.2f}")
        
        print(f"\nğŸ”— éš£æ¥ãƒãƒ¼ãƒ‰é–“ãƒ©ãƒ™ãƒ«ç›¸é–¢:")
        print(f"  æœ€ã‚‚é »ç¹ãªãƒ©ãƒ™ãƒ«ãƒšã‚¢:")
        # å®Ÿéš›ã®é »åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_pairs = sorted(
            analysis_result['pair_counts'].items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        for i, (pair, count) in enumerate(sorted_pairs[:5]):
            percentage = (count / analysis_result['total_edges']) * 100
            print(f"    {i+1}. ãƒ©ãƒ™ãƒ« {pair[0]} â†” ãƒ©ãƒ™ãƒ« {pair[1]}: {count:,} ã‚¨ãƒƒã‚¸ ({percentage:.1f}%)")
        
        print(f"\n  æœ€ã‚‚ç¨€ãªãƒ©ãƒ™ãƒ«ãƒšã‚¢:")
        for i, (pair, count) in enumerate(sorted_pairs[-5:]):
            percentage = (count / analysis_result['total_edges']) * 100
            print(f"    {i+1}. ãƒ©ãƒ™ãƒ« {pair[0]} â†” ãƒ©ãƒ™ãƒ« {pair[1]}: {count:,} ã‚¨ãƒƒã‚¸ ({percentage:.1f}%)")
    
    def analyze_all_datasets(self, save_plots=True, output_dir='./'):
        """
        ã™ã¹ã¦ã®ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†æ
        
        Args:
            save_plots (bool): ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        supported_datasets = get_supported_datasets()
        all_dataset_names = []
        
        for category, datasets in supported_datasets.items():
            all_dataset_names.extend(datasets)
        
        print(f"=== å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«ç›¸é–¢åˆ†æ ===")
        print(f"åˆ†æå¯¾è±¡: {len(all_dataset_names)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {', '.join(all_dataset_names)}")
        
        for dataset_name in all_dataset_names:
            try:
                self.analyze_dataset(dataset_name, save_plots, output_dir)
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼: {dataset_name} ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¯”è¼ƒ
        self._compare_all_datasets()
    
    def _compare_all_datasets(self):
        """
        å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµæœã‚’æ¯”è¼ƒ
        """
        if len(self.results) < 2:
            print("æ¯”è¼ƒã™ã‚‹ã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµæœãŒå¿…è¦ã§ã™")
            return
        
        print(f"\n=== å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¯”è¼ƒ ===")
        
        # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆæƒ…å ±ã‚’åé›†
        dataset_comparison = []
        
        for dataset_name, result in self.results.items():
            dataset_info = result['dataset_info']
            label_counts_list = list(result['label_counts'].values())
            
            # ã‚°ãƒ©ãƒ•å¯†åº¦ã‚’è¨ˆç®—
            density = (2 * dataset_info['num_edges']) / (dataset_info['num_nodes'] * (dataset_info['num_nodes'] - 1))
            
            # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡åº¦ã‚’è¨ˆç®—
            max_count = max(label_counts_list)
            min_count = min(label_counts_list)
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            
            # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒã®çµ±è¨ˆ
            mean_label_size = np.mean(label_counts_list)
            std_label_size = np.std(label_counts_list)
            
            # Homophilyã‚’è¨ˆç®—
            total_edges = result['total_edges']
            same_label_edges = 0
            for (label1, label2), count in result['pair_counts'].items():
                if label1 == label2:
                    same_label_edges += count
            
            homophily = same_label_edges / total_edges if total_edges > 0 else 0
            
            dataset_comparison.append({
                'dataset': dataset_name,
                'num_nodes': dataset_info['num_nodes'],
                'num_edges': dataset_info['num_edges'],
                'num_classes': dataset_info['num_classes'],
                'density': density,
                'imbalance_ratio': imbalance_ratio,
                'mean_label_size': mean_label_size,
                'std_label_size': std_label_size,
                'homophily': homophily
            })
        
        # çµæœã‚’è¡¨ç¤º
        print(f"{'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ':<12} {'ãƒãƒ¼ãƒ‰æ•°':<8} {'ã‚¨ãƒƒã‚¸æ•°':<8} {'ã‚¯ãƒ©ã‚¹æ•°':<6} {'å¯†åº¦':<8} {'ä¸å‡è¡¡æ¯”':<8} {'å¹³å‡ãƒ©ãƒ™ãƒ«':<10} {'åŒè³ªæ€§':<8}")
        print("-" * 70)
        
        for comp in sorted(dataset_comparison, key=lambda x: x['num_nodes'], reverse=True):
            print(f"{comp['dataset']:<12} {comp['num_nodes']:<8,} {comp['num_edges']:<8,} {comp['num_classes']:<6} "
                  f"{comp['density']:<8.6f} {comp['imbalance_ratio']:<8.2f} {comp['mean_label_size']:<10.1f} {comp['homophily']:.4f}")
        
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æ€§ã®è©³ç´°:")
        for comp in dataset_comparison:
            print(f"\n{comp['dataset']}:")
            print(f"  ãƒãƒ¼ãƒ‰æ•°: {comp['num_nodes']:,}")
            print(f"  ã‚¨ãƒƒã‚¸æ•°: {comp['num_edges']:,}")
            print(f"  ã‚¯ãƒ©ã‚¹æ•°: {comp['num_classes']}")
            print(f"  ã‚°ãƒ©ãƒ•å¯†åº¦: {comp['density']:.6f}")
            print(f"  ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡æ¯”: {comp['imbalance_ratio']:.2f}")
            print(f"  å¹³å‡ãƒ©ãƒ™ãƒ«ã‚µã‚¤ã‚º: {comp['mean_label_size']:.1f} Â± {comp['std_label_size']:.1f}")
            print(f"  åŒè³ªæ€§: {comp['homophily']:.4f}")


def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–
    analyzer = LabelCorrelationAnalyzer()
    
    # åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æŒ‡å®š
    # å€‹åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†æã™ã‚‹å ´åˆ
    # analyzer.analyze_dataset('Cora')
    # analyzer.analyze_dataset('Chameleon')
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†æã™ã‚‹å ´åˆ
    analyzer.analyze_all_datasets(save_plots=True, output_dir='./')


if __name__ == "__main__":
    main() 